{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0807b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 3: RAG Publishability Classifier Testing\n",
    "# Test the complete RAG-powered classification system\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from config.settings import *\n",
    "from src.rag_engine.rag_classifier import RAGPublishabilityClassifier\n",
    "\n",
    "print(\"Week 3: RAG Publishability Classifier Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: INITIALIZE CLASSIFIER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nSECTION 1: Initialize RAG Classifier\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Initialize the classifier\n",
    "print(\"Loading RAG Publishability Classifier...\")\n",
    "classifier = RAGPublishabilityClassifier()\n",
    "print(\"Classifier loaded successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: TEST WITH SAMPLE ARTICLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nSECTION 2: Test with Sample Articles\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "# Test articles of different quality levels\n",
    "test_articles = [\n",
    "    {\n",
    "        'label': 'High Quality',\n",
    "        'expected': 'موافق',\n",
    "        'text': \"\"\"\n",
    "        الرياض في 27 سبتمبر /واس/ أكد صاحب السمو الملكي الأمير محمد بن سلمان بن عبدالعزيز ولي العهد رئيس مجلس الوزراء، أهمية تعزيز التعاون الاستراتيجي بين المملكة العربية السعودية والولايات المتحدة الأمريكية في مختلف المجالات.\n",
    "        \n",
    "        جاء ذلك خلال استقبال سموه في قصر اليمامة بالرياض اليوم، وزير الخارجية الأمريكي أنتوني بلينكن والوفد المرافق له.\n",
    "        \n",
    "        وبحث الجانبان خلال اللقاء، أوجه التعاون الثنائي بين البلدين، والمستجدات الإقليمية والدولية ذات الاهتمام المشترك.\n",
    "        \n",
    "        وأشار ولي العهد إلى عمق العلاقات التاريخية بين المملكة والولايات المتحدة، مؤكداً حرص القيادة على تطوير هذه العلاقات في جميع المجالات.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'label': 'Low Quality',\n",
    "        'expected': 'مرفوض',\n",
    "        'text': \"\"\"\n",
    "        خبر مهم\n",
    "        حدث شيء اليوم\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'label': 'Medium Quality',\n",
    "        'expected': 'مرفوض',\n",
    "        'text': \"\"\"\n",
    "        أعلنت وزارة الصحة اليوم عن تسجيل حالات جديدة.\n",
    "        وأوضحت الوزارة أن الأرقام في تزايد مستمر.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'label': 'Good with Attribution',\n",
    "        'expected': 'موافق',\n",
    "        'text': \"\"\"\n",
    "        جدة في 27 سبتمبر /واس/ قال المتحدث باسم وزارة التعليم إن الوزارة أطلقت برنامجاً جديداً لتطوير المناهج الدراسية.\n",
    "        \n",
    "        وأضاف المتحدث أن البرنامج يهدف إلى تحسين جودة التعليم ومواكبة التطورات العالمية في هذا المجال.\n",
    "        \n",
    "        وأوضح أن البرنامج سيشمل جميع المراحل التعليمية ويتضمن تدريب المعلمين على أحدث الطرق التدريسية.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test each article\n",
    "test_results = []\n",
    "\n",
    "for i, test_case in enumerate(test_articles, 1):\n",
    "    print(f\"\\nTest Case {i}: {test_case['label']}\")\n",
    "    print(f\"Expected: {test_case['expected']}\")\n",
    "    print(f\"Text Preview: {test_case['text'].strip()[:100]}...\")\n",
    "    \n",
    "    # Classify the article\n",
    "    result = classifier.classify_article(test_case['text'])\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        decision = result['decision']\n",
    "        confidence = result['confidence']\n",
    "        reasoning = result['reasoning'][:150] + \"...\" if len(result['reasoning']) > 150 else result['reasoning']\n",
    "        \n",
    "        print(f\"AI Decision: {decision}\")\n",
    "        print(f\"Confidence: {confidence:.2f}\")\n",
    "        print(f\"Reasoning: {reasoning}\")\n",
    "        \n",
    "        # Check if correct\n",
    "        correct = decision == test_case['expected']\n",
    "        print(f\"Correct: {'✓' if correct else '✗'}\")\n",
    "        \n",
    "        test_results.append({\n",
    "            'label': test_case['label'],\n",
    "            'expected': test_case['expected'],\n",
    "            'predicted': decision,\n",
    "            'confidence': confidence,\n",
    "            'correct': correct\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        test_results.append({\n",
    "            'label': test_case['label'],\n",
    "            'expected': test_case['expected'],\n",
    "            'predicted': 'Error',\n",
    "            'confidence': 0.0,\n",
    "            'correct': False\n",
    "        })\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: TEST ON REAL DATASET SAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\\nSECTION 3: Test on Real Dataset Sample\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Load a small sample from the real dataset\n",
    "print(\"Loading real dataset sample...\")\n",
    "df_sample = pd.read_excel(MAIN_DATASET).head(10)\n",
    "df_sample['is_approved'] = df_sample['TrackId'].notna() & (df_sample['TrackId'] != 'NULL')\n",
    "\n",
    "real_test_results = []\n",
    "\n",
    "print(f\"Testing {len(df_sample)} real articles...\")\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Testing real articles\"):\n",
    "    article_text = row['Story']\n",
    "    actual_decision = 'موافق' if row['is_approved'] else 'مرفوض'\n",
    "    \n",
    "    # Classify with AI\n",
    "    result = classifier.classify_article(article_text)\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        predicted_decision = result['decision']\n",
    "        confidence = result['confidence']\n",
    "        correct = predicted_decision == actual_decision\n",
    "        \n",
    "        real_test_results.append({\n",
    "            'story_id': row['StoryId'],\n",
    "            'actual': actual_decision,\n",
    "            'predicted': predicted_decision,\n",
    "            'confidence': confidence,\n",
    "            'correct': correct,\n",
    "            'text_length': len(article_text)\n",
    "        })\n",
    "    else:\n",
    "        real_test_results.append({\n",
    "            'story_id': row['StoryId'],\n",
    "            'actual': actual_decision,\n",
    "            'predicted': 'Error',\n",
    "            'confidence': 0.0,\n",
    "            'correct': False,\n",
    "            'text_length': len(article_text)\n",
    "        })\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\\nSECTION 4: Results Analysis\")\n",
    "print(\"-\" * 31)\n",
    "\n",
    "# Sample test results\n",
    "print(\"Sample Test Results:\")\n",
    "correct_count = sum(1 for r in test_results if r['correct'])\n",
    "accuracy = correct_count / len(test_results) if test_results else 0\n",
    "print(f\"  Accuracy: {accuracy:.1%} ({correct_count}/{len(test_results)})\")\n",
    "\n",
    "avg_confidence = np.mean([r['confidence'] for r in test_results if r['confidence'] > 0])\n",
    "print(f\"  Average Confidence: {avg_confidence:.2f}\")\n",
    "\n",
    "# Real dataset results\n",
    "if real_test_results:\n",
    "    print(f\"\\nReal Dataset Results:\")\n",
    "    real_correct = sum(1 for r in real_test_results if r['correct'])\n",
    "    real_accuracy = real_correct / len(real_test_results)\n",
    "    print(f\"  Accuracy: {real_accuracy:.1%} ({real_correct}/{len(real_test_results)})\")\n",
    "    \n",
    "    real_avg_confidence = np.mean([r['confidence'] for r in real_test_results if r['confidence'] > 0])\n",
    "    print(f\"  Average Confidence: {real_avg_confidence:.2f}\")\n",
    "    \n",
    "    # Breakdown by actual decision\n",
    "    approved_articles = [r for r in real_test_results if r['actual'] == 'موافق']\n",
    "    rejected_articles = [r for r in real_test_results if r['actual'] == 'مرفوض']\n",
    "    \n",
    "    if approved_articles:\n",
    "        approved_accuracy = sum(1 for r in approved_articles if r['correct']) / len(approved_articles)\n",
    "        print(f\"  Approved Articles Accuracy: {approved_accuracy:.1%}\")\n",
    "    \n",
    "    if rejected_articles:\n",
    "        rejected_accuracy = sum(1 for r in rejected_articles if r['correct']) / len(rejected_articles)\n",
    "        print(f\"  Rejected Articles Accuracy: {rejected_accuracy:.1%}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\\nSECTION 5: Visualization\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "if real_test_results:\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy by decision type\n",
    "    actual_decisions = [r['actual'] for r in real_test_results]\n",
    "    predicted_decisions = [r['predicted'] for r in real_test_results]\n",
    "    \n",
    "    decision_counts = pd.DataFrame({\n",
    "        'Actual': actual_decisions,\n",
    "        'Predicted': predicted_decisions\n",
    "    })\n",
    "    \n",
    "    # Confusion matrix data\n",
    "    approved_actual = len([r for r in real_test_results if r['actual'] == 'موافق'])\n",
    "    rejected_actual = len([r for r in real_test_results if r['actual'] == 'مرفوض'])\n",
    "    \n",
    "    axes[0, 0].bar(['Approved Actual', 'Rejected Actual'], [approved_actual, rejected_actual], \n",
    "                   color=['green', 'red'], alpha=0.7)\n",
    "    axes[0, 0].set_title('Actual Decisions Distribution')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Confidence distribution\n",
    "    confidences = [r['confidence'] for r in real_test_results if r['confidence'] > 0]\n",
    "    if confidences:\n",
    "        axes[0, 1].hist(confidences, bins=10, alpha=0.7, color='blue')\n",
    "        axes[0, 1].set_title('Confidence Score Distribution')\n",
    "        axes[0, 1].set_xlabel('Confidence')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Accuracy vs confidence\n",
    "    correct_confidences = [r['confidence'] for r in real_test_results if r['correct']]\n",
    "    incorrect_confidences = [r['confidence'] for r in real_test_results if not r['correct']]\n",
    "    \n",
    "    if correct_confidences and incorrect_confidences:\n",
    "        axes[1, 0].hist([correct_confidences, incorrect_confidences], \n",
    "                       bins=10, alpha=0.7, label=['Correct', 'Incorrect'], \n",
    "                       color=['green', 'red'])\n",
    "        axes[1, 0].set_title('Confidence by Correctness')\n",
    "        axes[1, 0].set_xlabel('Confidence')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # Text length vs accuracy\n",
    "    text_lengths = [r['text_length'] for r in real_test_results]\n",
    "    correctness = [1 if r['correct'] else 0 for r in real_test_results]\n",
    "    \n",
    "    axes[1, 1].scatter(text_lengths, correctness, alpha=0.6)\n",
    "    axes[1, 1].set_title('Text Length vs Accuracy')\n",
    "    axes[1, 1].set_xlabel('Text Length (characters)')\n",
    "    axes[1, 1].set_ylabel('Correct (1) / Incorrect (0)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SYSTEM PERFORMANCE\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\\nSECTION 6: System Performance Summary\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "performance_summary = {\n",
    "    'Vector Database': f\"{classifier.vector_store.get_collection_stats()['approved_articles']['count']} approved + {classifier.vector_store.get_collection_stats()['rejected_articles']['count']} rejected examples\",\n",
    "    'LLM Model': classifier.model,\n",
    "    'Sample Test Accuracy': f\"{accuracy:.1%}\" if test_results else \"N/A\",\n",
    "    'Real Dataset Accuracy': f\"{real_accuracy:.1%}\" if real_test_results else \"N/A\",\n",
    "    'Average Confidence': f\"{real_avg_confidence:.2f}\" if real_test_results else \"N/A\"\n",
    "}\n",
    "\n",
    "print(\"Performance Summary:\")\n",
    "for key, value in performance_summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nWeek 3 Status: RAG Classifier Implementation Complete!\")\n",
    "print(\"Next: Optimize accuracy and build Stage 2 (Editorial Assistant)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\\nSECTION 7: Save Results\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Save test results\n",
    "if real_test_results:\n",
    "    results_df = pd.DataFrame(real_test_results)\n",
    "    output_file = ANALYTICS_DIR / \"week3_rag_classifier_results.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(\"Week 3 testing complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
