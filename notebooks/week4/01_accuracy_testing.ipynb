{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Week 4: RAG Classifier Accuracy Testing and Optimization\n",
    "Test the classifier against known decisions and optimize parameters\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from config.settings import *\n",
    "from src.rag_engine.rag_classifier import RAGPublishabilityClassifier\n",
    "from src.rag_engine.vector_store import NewsVectorStore\n",
    "\n",
    "class AccuracyTester:\n",
    "    def __init__(self):\n",
    "        self.classifier = RAGPublishabilityClassifier()\n",
    "        self.results = []\n",
    "        \n",
    "    def load_test_data(self, sample_size=100):\n",
    "        \"\"\"Load balanced test set of approved/rejected articles\"\"\"\n",
    "        \n",
    "        # Load main dataset\n",
    "        df = pd.read_excel(MAIN_DATASET)\n",
    "        df['is_approved'] = df['TrackId'].notna() & (df['TrackId'] != 'NULL')\n",
    "        \n",
    "        # Create balanced test set\n",
    "        approved = df[df['is_approved']].sample(n=min(sample_size//2, 50))\n",
    "        rejected = df[~df['is_approved']].sample(n=min(sample_size//2, 50))\n",
    "        \n",
    "        test_data = pd.concat([approved, rejected]).reset_index(drop=True)\n",
    "        test_data['expected_decision'] = test_data['is_approved'].map({True: 'موافق', False: 'مرفوض'})\n",
    "        \n",
    "        return test_data\n",
    "        \n",
    "    def test_classifier_accuracy(self, test_data, save_results=True):\n",
    "        \"\"\"Test classifier on known decisions\"\"\"\n",
    "        \n",
    "        print(f\"Testing classifier accuracy on {len(test_data)} articles...\")\n",
    "        \n",
    "        results = []\n",
    "        processing_times = []\n",
    "        \n",
    "        for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Get classifier prediction\n",
    "                result = self.classifier.classify_article(row['Story'])\n",
    "                \n",
    "                processing_time = time.time() - start_time\n",
    "                processing_times.append(processing_time)\n",
    "                \n",
    "                # Store results\n",
    "                test_result = {\n",
    "                    'article_id': row['StoryId'],\n",
    "                    'expected_decision': row['expected_decision'],\n",
    "                    'predicted_decision': result['decision'],\n",
    "                    'confidence': result['confidence'],\n",
    "                    'processing_time': processing_time,\n",
    "                    'correct': result['decision'] == row['expected_decision'],\n",
    "                    'article_length': len(str(row['Story'])),\n",
    "                    'reasoning': result['reasoning'][:200] if result['reasoning'] else ''\n",
    "                }\n",
    "                \n",
    "                results.append(test_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {row['StoryId']}: {e}\")\n",
    "                results.append({\n",
    "                    'article_id': row['StoryId'],\n",
    "                    'expected_decision': row['expected_decision'],\n",
    "                    'predicted_decision': 'ERROR',\n",
    "                    'confidence': 0.0,\n",
    "                    'processing_time': 0.0,\n",
    "                    'correct': False,\n",
    "                    'article_length': len(str(row['Story'])),\n",
    "                    'reasoning': f'Error: {e}'\n",
    "                })\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Filter out errors for accuracy calculation\n",
    "        valid_results = results_df[results_df['predicted_decision'] != 'ERROR']\n",
    "        \n",
    "        if len(valid_results) > 0:\n",
    "            accuracy = valid_results['correct'].mean()\n",
    "            avg_confidence = valid_results['confidence'].mean()\n",
    "            avg_processing_time = np.mean(processing_times) if processing_times else 0\n",
    "            \n",
    "            # Separate metrics for approved/rejected\n",
    "            approved_results = valid_results[valid_results['expected_decision'] == 'موافق']\n",
    "            rejected_results = valid_results[valid_results['expected_decision'] == 'مرفوض']\n",
    "            \n",
    "            approved_accuracy = approved_results['correct'].mean() if len(approved_results) > 0 else 0\n",
    "            rejected_accuracy = rejected_results['correct'].mean() if len(rejected_results) > 0 else 0\n",
    "            \n",
    "            metrics = {\n",
    "                'overall_accuracy': accuracy,\n",
    "                'approved_accuracy': approved_accuracy,\n",
    "                'rejected_accuracy': rejected_accuracy,\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'avg_processing_time': avg_processing_time,\n",
    "                'total_tested': len(results),\n",
    "                'valid_predictions': len(valid_results),\n",
    "                'errors': len(results) - len(valid_results)\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\n=== ACCURACY TEST RESULTS ===\")\n",
    "            print(f\"Overall Accuracy: {accuracy:.2%}\")\n",
    "            print(f\"Approved Articles Accuracy: {approved_accuracy:.2%}\")\n",
    "            print(f\"Rejected Articles Accuracy: {rejected_accuracy:.2%}\")\n",
    "            print(f\"Average Confidence: {avg_confidence:.2f}\")\n",
    "            print(f\"Average Processing Time: {avg_processing_time:.2f}s\")\n",
    "            print(f\"Articles per Hour: {3600/avg_processing_time:.0f}\" if avg_processing_time > 0 else \"N/A\")\n",
    "            print(f\"Errors: {metrics['errors']}/{len(results)}\")\n",
    "            \n",
    "            # Save results\n",
    "            if save_results:\n",
    "                results_df.to_csv(ANALYTICS_DIR / 'week4_accuracy_test.csv', index=False)\n",
    "                \n",
    "                with open(ANALYTICS_DIR / 'week4_metrics.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "                    \n",
    "                print(f\"\\nResults saved to {ANALYTICS_DIR}\")\n",
    "            \n",
    "            self.results = results_df\n",
    "            return metrics\n",
    "        else:\n",
    "            print(\"No valid predictions to analyze!\")\n",
    "            return None\n",
    "            \n",
    "    def analyze_errors(self):\n",
    "        \"\"\"Analyze incorrect predictions to find patterns\"\"\"\n",
    "        \n",
    "        if len(self.results) == 0:\n",
    "            print(\"No results to analyze. Run test_classifier_accuracy first.\")\n",
    "            return\n",
    "            \n",
    "        incorrect = self.results[~self.results['correct']]\n",
    "        \n",
    "        if len(incorrect) == 0:\n",
    "            print(\"Perfect accuracy! No errors to analyze.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\n=== ERROR ANALYSIS ===\")\n",
    "        print(f\"Total errors: {len(incorrect)}/{len(self.results)}\")\n",
    "        \n",
    "        # Analyze by expected vs predicted\n",
    "        error_types = incorrect.groupby(['expected_decision', 'predicted_decision']).size()\n",
    "        print(f\"\\nError breakdown:\")\n",
    "        for (expected, predicted), count in error_types.items():\n",
    "            print(f\"  Expected {expected}, Got {predicted}: {count}\")\n",
    "            \n",
    "        # Analyze by confidence\n",
    "        low_confidence_errors = incorrect[incorrect['confidence'] < 0.5]\n",
    "        print(f\"\\nLow confidence errors (<0.5): {len(low_confidence_errors)}\")\n",
    "        \n",
    "        # Analyze by article length\n",
    "        print(f\"\\nAverage length of incorrect predictions: {incorrect['article_length'].mean():.0f}\")\n",
    "        print(f\"Average length of correct predictions: {self.results[self.results['correct']]['article_length'].mean():.0f}\")\n",
    "        \n",
    "        return incorrect\n",
    "        \n",
    "    def optimize_retrieval_parameters(self, test_data):\n",
    "        \"\"\"Test different RAG retrieval parameters\"\"\"\n",
    "        \n",
    "        print(\"Optimizing RAG retrieval parameters...\")\n",
    "        \n",
    "        # Test different k values (number of similar articles to retrieve)\n",
    "        k_values = [3, 5, 7, 10]\n",
    "        results = []\n",
    "        \n",
    "        for k in k_values:\n",
    "            print(f\"Testing k={k}...\")\n",
    "            \n",
    "            # Update classifier retrieval parameter\n",
    "            original_k = self.classifier.retrieval_k\n",
    "            self.classifier.retrieval_k = k\n",
    "            \n",
    "            # Test on small sample\n",
    "            sample_data = test_data.sample(n=min(20, len(test_data)))\n",
    "            metrics = self.test_classifier_accuracy(sample_data, save_results=False)\n",
    "            \n",
    "            if metrics:\n",
    "                results.append({\n",
    "                    'k': k,\n",
    "                    'accuracy': metrics['overall_accuracy'],\n",
    "                    'avg_confidence': metrics['avg_confidence'],\n",
    "                    'avg_time': metrics['avg_processing_time']\n",
    "                })\n",
    "            \n",
    "            # Restore original\n",
    "            self.classifier.retrieval_k = original_k\n",
    "            \n",
    "        # Find best k\n",
    "        if results:\n",
    "            best_result = max(results, key=lambda x: x['accuracy'])\n",
    "            print(f\"\\nBest k value: {best_result['k']} (accuracy: {best_result['accuracy']:.2%})\")\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        return None\n",
    "\n",
    "def run_week4_accuracy_tests():\n",
    "    \"\"\"Complete Week 4 accuracy testing workflow\"\"\"\n",
    "    \n",
    "    print(\"Starting Week 4 Accuracy Testing...\")\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = AccuracyTester()\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = tester.load_test_data(sample_size=100)\n",
    "    print(f\"Loaded test dataset: {len(test_data)} articles\")\n",
    "    \n",
    "    # Run accuracy test\n",
    "    metrics = tester.test_classifier_accuracy(test_data)\n",
    "    \n",
    "    if metrics:\n",
    "        # Analyze errors\n",
    "        tester.analyze_errors()\n",
    "        \n",
    "        # Optimize parameters if accuracy is below target\n",
    "        if metrics['overall_accuracy'] < 0.85:\n",
    "            print(f\"\\nAccuracy {metrics['overall_accuracy']:.2%} below target 85%. Optimizing...\")\n",
    "            tester.optimize_retrieval_parameters(test_data)\n",
    "        else:\n",
    "            print(f\"\\nAccuracy target met: {metrics['overall_accuracy']:.2%} >= 85%\")\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metrics = run_week4_accuracy_tests()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
